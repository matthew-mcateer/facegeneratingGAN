{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Generation\n",
    "\n",
    "This notebook creates a Genrative Adversarial Network (GAN). The GAN is designed to produce realistic human faces after being trained on a large data set of human faces. Prior to this, it's functionality is tested by training on a database of handwriting sample and then producing realistic images of had-drawn alphanumeric characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Relevant Packages\n",
    "The packages used in this are [os](https://docs.python.org/3/library/os.html), [glob](https://docs.python.org/3/library/glob.html), [matplotlib](https://matplotlib.org/), [numpy](http://www.numpy.org/), [tqdm](https://pypi.python.org/pypi/tqdm), [warnings](https://docs.python.org/3.1/library/warnings.html), and [tensorflow](https://www.tensorflow.org/api_docs/python/)\n",
    "\n",
    "helper and unittests are dependencies that should be included in the same directory as this Jupyter notebook.\n",
    "\n",
    "Documentation for the packages is included in the hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import helper\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import unittests as tests\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from distutils.version import LooseVersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data\n",
    "This GAN utilizes the MNIST and CelebA datasets. The MNIST data set is far simpler than the the celebA dataset. The MNIST is also black ans white instead of in RGB. Running the GAN on MNIST allows for testing of how well the model trains before moving onto the more complex data.\n",
    "\n",
    "If you're using [FloydHub](https://www.floydhub.com/), set `data_dir` to \"/input\" and use the [FloydHub data ID](http://docs.floydhub.com/home/using_datasets/) \"R5KrjnANiKVhLWAkpXhNBe\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using FloydHub, use with data ID \"R5KrjnANiKVhLWAkpXhNBe\"\n",
    "data_dir = '/input'\n",
    "\n",
    "# If using Local data, use the /data subfolder \n",
    "#data_dir = './data'\n",
    "\n",
    "helper.download_extract('mnist', data_dir)\n",
    "helper.download_extract('celeba', data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration and Preprocessing of the Data\n",
    "The values of the MNIST and CelebA dataset will be in the range of -0.5 to 0.5 of 28x28 dimensional images. The CelebA images will be cropped to remove parts of the image that don't include a face, then resized down to 28x28.\n",
    "\n",
    "The MNIST images are black and white images with a single color channel while the CelebA images have 3 color channels.\n",
    "\n",
    "### MNIST\n",
    "The [MNIST](http://yann.lecun.com/exdb/mnist/) dataset contains images of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n_images = 25\n",
    "mnist_images = helper.get_batch(glob(os.path.join(data_dir, 'mnist/*.jpg'))[:show_n_images], 28, 28, 'L')\n",
    "plt.imshow(helper.images_square_grid(mnist_images, 'L'), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CelebA\n",
    "The [CelebFaces Attributes Dataset (CelebA)](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) is a dataset that contains over 202,599 celebrity images with annotations. For the purposes of generating new faces, the annotations are irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n_images = 25\n",
    "\n",
    "mnist_images = helper.get_batch(glob(os.path.join(data_dir, 'img_align_celeba/*.jpg'))[:show_n_images], 28, 28, 'RGB')\n",
    "plt.imshow(helper.images_square_grid(mnist_images, 'RGB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network\n",
    "The following functions defined below are used to build the neural network:\n",
    "* model_inputs\n",
    "* discriminator\n",
    "* generator\n",
    "* model_loss\n",
    "* model_opt\n",
    "* train\n",
    "\n",
    "### Checking the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Checks TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Checks for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train the neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "The function model_inputs() creates the model inputs. it returns a tuple of (tensor of real input images, tensor of z data, learning rate)\n",
    "* image_width: The input image width\n",
    "* image_height: The input image height\n",
    "* z_dim: The dimension of Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs(image_width, image_height, image_channels, z_dim):\n",
    "    tensor_real_inputs = tf.placeholder(dtype = tf.float32, shape = (None, image_height, image_width, image_channels))\n",
    "    tensor_z_data = tf.placeholder(dtype = tf.float32, shape = (None, z_dim))\n",
    "    learning_rate = tf.placeholder(dtype = tf.float32, shape = ())\n",
    "\n",
    "    return tensor_real_inputs, tensor_z_data, learning_rate\n",
    "\n",
    "tests.test_model_inputs(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "The discriminator() function creates the discriminator network. It returns a tuple of the tensor output of the discriminator and the tensor logits of the discriminator.\n",
    "* images: Tensor of input image(s)\n",
    "* reuse: Boolean if the weights should be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(images, reuse = False):\n",
    "    with tf.variable_scope('discriminator', reuse = reuse):\n",
    "        x = tf.layers.conv2d(images, filters = 32, \n",
    "                             kernel_size = 5, \n",
    "                             strides = 2,\n",
    "                             padding='same', \n",
    "                             kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        x = tf.maximum(0.2 * x, x)\n",
    "        \n",
    "        x = tf.layers.conv2d(images, filters = 64, \n",
    "                             kernel_size = 5, \n",
    "                             strides = 2,\n",
    "                             padding='same', \n",
    "                             kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        x = tf.layers.batch_normalization(x, training = True)\n",
    "        x = tf.maximum(0.2 * x, x)\n",
    "        x = tf.nn.dropout(x, keep_prob = 0.5)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters = 128, \n",
    "                             kernel_size = 5, \n",
    "                             strides = 2,\n",
    "                             padding = 'same', \n",
    "                             kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        x = tf.layers.batch_normalization(x, training = True)\n",
    "        x = tf.maximum(0.2 * x, x)\n",
    "        x = tf.nn.dropout(x, keep_prob = 0.5)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, filters = 256, \n",
    "                             kernel_size = 5, \n",
    "                             strides = 2,\n",
    "                             padding = 'same', \n",
    "                             kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        x = tf.layers.batch_normalization(x, training = True)\n",
    "        x = tf.maximum(0.2 * x, x)\n",
    "        \n",
    "        flattened = tf.reshape(x, [-1, 2 * 2 * 256])\n",
    "        logits = tf.layers.dense(flattened, 1, activation = None)\n",
    "        output = tf.sigmoid(logits)\n",
    "\n",
    "    return output, logits\n",
    "\n",
    "tests.test_discriminator(discriminator, tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "The generator() function creates the generator network. It returns the tensor output of the generator\n",
    "* z: Input z\n",
    "* out_channel_dim: The number of channels in the output image\n",
    "* is_train: Boolean if generator is being used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, out_channel_dim, is_train=True):\n",
    "    \n",
    "    with tf.variable_scope('generator', reuse = not is_train):\n",
    "        x = tf.layers.dense(z, units = 4 * 4 * 512)\n",
    "        x = tf.reshape(x, (-1, 4, 4, 512))\n",
    "        x = tf.layers.batch_normalization(x, training = is_train)\n",
    "        x = tf.maximum( 0.2 * x, x)\n",
    "        \n",
    "        x = tf.layers.conv2d_transpose(x, filters = 128, kernel_size = 4, strides = 1, padding = 'valid')\n",
    "        x = tf.layers.batch_normalization(x, training = is_train)\n",
    "        x = tf.maximum(0.2 * x, x)\n",
    "        \n",
    "        x = tf.layers.conv2d_transpose(x, filters = 64, kernel_size = 5, strides = 2, padding = 'same')\n",
    "        x = tf.layers.batch_normalization(x, training = is_train)\n",
    "        x = tf.maximum(0.2 * x, x)\n",
    "        \n",
    "        x = tf.layers.conv2d_transpose(x, filters = 32, kernel_size = 5, strides = 2, padding = 'same')\n",
    "        x = tf.layers.batch_normalization(x, training = is_train)\n",
    "        x = tf.maximum(0.2 * x, x)\n",
    "        \n",
    "        logits = tf.layers.conv2d_transpose(x, filters = out_channel_dim, kernel_size = 3, strides = 1,\n",
    "                                            padding = 'same')\n",
    "        out = tf.tanh(logits)\n",
    "        print(out.get_shape())\n",
    "        \n",
    "    return out\n",
    "\n",
    "tests.test_generator(generator, tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "The function model_loss() gets the loss for the discriminator and generator. It returns a tuple of the discriminator loss and the generator loss.\n",
    "* input_real: Images from the real dataset\n",
    "* input_z: Z input\n",
    "* out_channel_dim: The number of channels in the output image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, out_channel_dim):\n",
    "    \n",
    "    generator_model = generator(input_z, out_channel_dim, is_train = True)\n",
    "    discriminator_model_real, discriminator_logits_real = discriminator(input_real, reuse = False)\n",
    "    discriminator_model_fake, discriminator_logits_fake = discriminator(generator_model, reuse = True)\n",
    "    \n",
    "    discriminator_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = discriminator_logits_real, labels = tf.ones_like(discriminator_model_real) * (1 - 0.1)))\n",
    "    \n",
    "    discriminator_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = discriminator_logits_fake, labels = tf.zeros_like(discriminator_model_fake)))\n",
    "    \n",
    "    generator_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = discriminator_logits_fake, labels = tf.ones_like(discriminator_model_fake)))\n",
    "    \n",
    "    discriminator_loss = discriminator_loss_real + discriminator_loss_fake\n",
    "    \n",
    "    return discriminator_loss, generator_loss\n",
    "\n",
    "tests.test_model_loss(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "The function model_opt() gets the optimization operations. it returns a tuple of the discriminator training operation and the generator training operation)\n",
    "* d_loss: Discriminator loss Tensor\n",
    "* g_loss: Generator loss Tensor\n",
    "* learning_rate: Learning Rate Placeholder\n",
    "* beta1: The exponential decay rate for the 1st moment in the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, learning_rate, beta1):\n",
    "    training_vars = tf.trainable_variables()\n",
    "    discriminator_vars = [var for var in training_vars if var.name.startswith('discriminator')]\n",
    "    generator_vars = [var for var in training_vars if var.name.startswith('generator')]\n",
    "    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    discriminator_updates = [opt for opt in update_ops if opt.name.startswith('discriminator')]\n",
    "    generator_updates = [opt for opt in update_ops if opt.name.startswith('generator')]\n",
    "\n",
    "    with tf.control_dependencies(discriminator_updates):\n",
    "        discriminator_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate, beta1 = beta1).minimize(d_loss, var_list = discriminator_vars)\n",
    "\n",
    "    with tf.control_dependencies(generator_updates):\n",
    "        generator_opt = tf.train.AdamOptimizer(\n",
    "            learning_rate = learning_rate, beta1 = beta1).minimize(g_loss, var_list = generator_vars)\n",
    "            \n",
    "    return discriminator_opt, generator_opt\n",
    "\n",
    "tests.test_model_opt(model_opt, tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Showing Generator Output\n",
    "The show_generator_output() function shows an example output for the generator.\n",
    "* sess: TensorFlow session\n",
    "* n_images: Number of Images to display\n",
    "* input_z: Input Z Tensor\n",
    "* out_channel_dim: The number of channels in the output image\n",
    "* image_mode: The mode to use for images (\"RGB\" or \"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_generator_output(sess, n_images, input_z, out_channel_dim, image_mode):\n",
    "    cmap = None if image_mode == 'RGB' else 'gray'\n",
    "    z_dim = input_z.get_shape().as_list()[-1]\n",
    "    example_z = np.random.uniform(-1, 1, size=[n_images, z_dim])\n",
    "\n",
    "    samples = sess.run(\n",
    "        generator(input_z, out_channel_dim, False),\n",
    "        feed_dict={input_z: example_z})\n",
    "\n",
    "    images_grid = helper.images_square_grid(samples, image_mode)\n",
    "    plt.imshow(images_grid, cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training GANs\n",
    "The train() function trains the GAN.\n",
    "* epoch_count: Number of epochs\n",
    "* batch_size: Batch Size\n",
    "* z_dim: Z dimension\n",
    "* learning_rate: Learning Rate\n",
    "* beta1: The exponential decay rate for the 1st moment in the optimizer\n",
    "* get_batches: Function to get batches\n",
    "* data_shape: Shape of the data\n",
    "* data_image_mode: The image mode to use for images (\"RGB\" or \"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch_count, batch_size, z_dim, learning_rate, beta1, get_batches, data_shape, data_image_mode):\n",
    "    n_samples, width, height, channels = data_shape\n",
    "    input_real, input_z, lr = model_inputs(width, height, channels, z_dim)\n",
    "    discriminator_loss, generator_loss = model_loss(input_real, input_z, channels)\n",
    "    discriminator_train_opt, generator_train_opt = model_opt(discriminator_loss, generator_loss, lr, beta1)\n",
    "    print(data_shape)\n",
    "    \n",
    "    i = 0    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch_i in range(epoch_count):\n",
    "            for batch_images in get_batches(batch_size):           \n",
    "                batch_images = batch_images * 2.0\n",
    "                batch_z = np.random.uniform(-1, 1, size = (batch_size, z_dim))\n",
    "                \n",
    "                sess.run(discriminator_train_opt, feed_dict = {input_real: batch_images,\n",
    "                                                               input_z: batch_z,\n",
    "                                                               lr: learning_rate})\n",
    "                sess.run(generator_train_opt, feed_dict = {input_real: batch_images,\n",
    "                                                           input_z: batch_z,\n",
    "                                                           lr: learning_rate})\n",
    "                \n",
    "                i += 1\n",
    "                if i % 10 == 0:\n",
    "                    train_loss_discriminator = discriminator_loss.eval({input_z: batch_z, input_real: batch_images})\n",
    "                    train_loss_generator = generator_loss.eval({input_z: batch_z})\n",
    "                    print('Epoch %d/%d Discriminator loss %.4f Generator loss %.4f' % (epoch_i + 1,\n",
    "                                                                                       epoch_count,\n",
    "                                                                                       train_loss_discriminator,\n",
    "                                                                                       train_loss_generator))\n",
    "                if i % 100 == 0:\n",
    "                    show_generator_output(sess, 50, input_z, channels, data_image_mode)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing GAN Architecture on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "z_dim = 128\n",
    "learning_rate = 0.0005\n",
    "beta1 = 0.1\n",
    "epochs = 2\n",
    "\n",
    "# After 2 epochs, the GANs are usually able to generate images that look like handwritten digits.\n",
    "# When running, make sure the loss of the generator is lower than the loss of the discriminator or close to 0.\n",
    "\n",
    "mnist_dataset = helper.Dataset('mnist', glob(os.path.join(data_dir, 'mnist/*.jpg')))\n",
    "with tf.Graph().as_default():\n",
    "    train(epochs, batch_size, z_dim, learning_rate, beta1, mnist_dataset.get_batches,\n",
    "          mnist_dataset.shape, mnist_dataset.image_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running GANs on CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "z_dim = 128\n",
    "learning_rate = 0.0002\n",
    "beta1 = 0.5\n",
    "epochs = 1\n",
    "# ~20 minutes for typical GPU to run one epoch\n",
    "\n",
    "celeba_dataset = helper.Dataset('celeba', glob(os.path.join(data_dir, 'img_align_celeba/*.jpg')))\n",
    "with tf.Graph().as_default():\n",
    "    train(epochs, batch_size, z_dim, learning_rate, beta1, celeba_dataset.get_batches,\n",
    "          celeba_dataset.shape, celeba_dataset.image_mode)\n",
    "    \n",
    "# If cost of GPU time is an issue, the GAN can be paused prematurely when the faces look realistic enough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
